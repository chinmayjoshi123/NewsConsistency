{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed \n",
    "\n",
    "import backbone as backbone_models\n",
    "from models import get_lewel_model\n",
    "from utils import utils, lr_schedule, LARS, get_norm\n",
    "import data.transforms as data_transforms\n",
    "from engine import ss_validate,validate\n",
    "from data.base_dataset import get_dataset\n",
    "backbone_model_names = sorted(name for name in backbone_models.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "    and callable(backbone_models.__dict__[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "confs = {\n",
    "    \"local_rank\":0,\n",
    "    \"dataset\" : \"newsclippings\",\n",
    "    \"data-root\":r\"E:/Chinmay/news_clippings\",\n",
    "    #\"data-root\":r'/projects/academic/sreyasee/msmu',\n",
    "    \"arch\":\"LEWELB\",\n",
    "    \"backbone\":\"clip_encoder\",\n",
    "    \"workers\":8,\n",
    "    \"epochs\": 150,\n",
    "    \"start-epoch\": 0,\n",
    "    \"warmup-epoch\":10,\n",
    "    \"batch-size\":16,\n",
    "    \"learning_rate\":1e-3,\n",
    "    \"schedule\":[120,160],\n",
    "    \"cos\":True,\n",
    "    \"momentum\":0.9,\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"save-dir\":r'/projects/academic/sreyasee/chinmayd/saved_models',\n",
    "    \"print_freq\":200,\n",
    "    \"save_freq\":10,\n",
    "    \"eval_freq\":1,\n",
    "    \"resume\":None,\n",
    "    \"pretrained\":\"\",\n",
    "    \"super-pretrained\":\"\",\n",
    "    \"evaluate\":False,\n",
    "    \"world-size\":1,\n",
    "    \"rank\":0,\n",
    "    \"dist-url\":\"tcp://224.66.41.62:23456\",\n",
    "    \"dist-backend\":\"nccl\",\n",
    "    \"seed\":\"23456\",\n",
    "    \"gpu\":None,\n",
    "    \"port\":5389,\n",
    "    \"multiprocessing-distributed\": None,\n",
    "    \"proj_dim\":256,\n",
    "    \"enc-m\":0.996,\n",
    "    \"norm\":None, \n",
    "    \"num-neck-mlp\":2,\n",
    "    \"hid-dim\":4096,\n",
    "    \"amp\": None, \n",
    "    \"lewel-l2-norm\": True,\n",
    "    \"lewel-scale\":1,\n",
    "    \"lewel-num-heads\":4,\n",
    "    \"lewel-loss-weight\":0.5,\n",
    "    \"num-nn\": 20 ,\n",
    "    \"nn-mem-percent\":0.1,\n",
    "    \"nn-query-percent\":0.5,\n",
    "    \"scale\":1\n",
    "}\n",
    "best_acc1 = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global best_acc1\n",
    "    # create model\n",
    "    print(\"=> creating model '{}' with backbone '{}'\".format(confs[\"arch\"], confs[\"backbone\"]))\n",
    "    model_func = get_lewel_model(confs[\"arch\"])\n",
    "    norm_layer = get_norm(confs[\"norm\"])\n",
    "    model = model_func(\n",
    "        dim=confs[\"proj_dim\"],\n",
    "        m=confs[\"enc-m\"],\n",
    "        hid_dim=confs[\"hid-dim\"],\n",
    "        num_neck_mlp=confs[\"num-neck-mlp\"],\n",
    "        scale=confs[\"scale\"],\n",
    "        l2_norm=confs[\"lewel-l2-norm\"],\n",
    "        num_heads=confs[\"lewel-num-heads\"],\n",
    "        loss_weight=confs[\"lewel-loss-weight\"],\n",
    "    )\n",
    "    print(model)\n",
    "    print(confs)\n",
    "\n",
    "    if confs[\"pretrained\"]:\n",
    "        if os.path.isfile(confs[\"pretrained\"]):\n",
    "            print(\"=> loading pretrained model from '{}'\".format(confs[\"pretrained\"]))\n",
    "            state_dict = torch.load(confs[\"pretrained\"], map_location=\"cpu\")['state_dict']\n",
    "            # rename state_dict keys\n",
    "            for k in list(state_dict.keys()):\n",
    "                new_key = k.replace(\"module.\", \"\")\n",
    "                state_dict[new_key] = state_dict[k]\n",
    "                del state_dict[k]\n",
    "            msg = model.load_state_dict(state_dict, strict=False)\n",
    "            print(\"=> loaded pretrained model from '{}'\".format(confs[\"pretrained\"]))\n",
    "            if len(msg.missing_keys) > 0:\n",
    "                print(\"missing keys: {}\".format(msg.missing_keys))\n",
    "            if len(msg.unexpected_keys) > 0:\n",
    "                print(\"unexpected keys: {}\".format(msg.unexpected_keys))\n",
    "        else:\n",
    "            print(\"=> no pretrained model found at '{}'\".format(confs[\"pretrained\"]))\n",
    "\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    \n",
    "    confs[\"batch-size\"] = int(confs[\"batch-size\"]/ confs[\"world-size\"])\n",
    "    confs[\"workers\"] = int((confs[\"workers\"] + confs[\"world-size\"] - 1) / confs[\"world-size\"])\n",
    "\n",
    "    # define optimizer\n",
    "    params = collect_params(model, exclude_bias_and_bn=True, sync_bn='EMAN' in confs[\"arch\"])\n",
    "    optimizer = LARS(params, lr=confs[\"learning_rate\"], momentum=confs[\"momentum\"], weight_decay=confs[\"weight_decay\"])\n",
    "    scaler = torch.cuda.amp.GradScaler() if confs[\"amp\"] else None\n",
    "\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    if confs[\"resume\"]:\n",
    "        if os.path.isfile(confs[\"resume\"]):\n",
    "            print(\"=> loading checkpoint '{}'\".format(confs[\"resume\"]))\n",
    "            if confs[\"gpu\"] is None:\n",
    "                checkpoint = torch.load(confs[\"resume\"])\n",
    "            else:\n",
    "                # Map model to be loaded to specified single gpu.\n",
    "                loc = 'cuda'\n",
    "                checkpoint = torch.load(confs[\"resume\"], map_location=loc)\n",
    "            confs[\"start-epoch\"] = checkpoint['epoch']\n",
    "            if 'best_acc1' in checkpoint:\n",
    "                best_acc1 = checkpoint['best_acc1']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(confs[\"resume\"], checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(confs[\"resume\"]))\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # Data loading code\n",
    "    transform1, transform2 = data_transforms.get_byol_tranforms()\n",
    "    train_dataset = get_dataset(\n",
    "        confs[\"dataset\"],\n",
    "        mode='train',\n",
    "        transform=data_transforms.TwoCropsTransform(transform1, transform2),\n",
    "        data_root=confs[\"data-root\"])\n",
    "    print(\"train_dataset:\\n{}\".format(train_dataset))\n",
    "    train_sampler = None\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=confs[\"batch-size\"], shuffle=(train_sampler is None),\n",
    "        num_workers=confs[\"workers\"], pin_memory=True, sampler=train_sampler, drop_last=True,\n",
    "        persistent_workers=True)\n",
    "\n",
    "    val_loader_base = torch.utils.data.DataLoader(\n",
    "        get_dataset(\n",
    "            confs[\"dataset\"],\n",
    "            mode='eval',\n",
    "            transform=data_transforms.TwoCropsTransform(transform1, transform2),\n",
    "            data_root=confs[\"data-root\"],\n",
    "        ),\n",
    "        batch_size=confs[\"batch-size\"], shuffle=False,\n",
    "        num_workers=confs[\"workers\"]//2, pin_memory=True,\n",
    "        persistent_workers=True)\n",
    "\n",
    "    val_loader_query = torch.utils.data.DataLoader(\n",
    "        get_dataset(\n",
    "            confs[\"dataset\"],\n",
    "            mode='val',\n",
    "            transform=data_transforms.TwoCropsTransform(transform1, transform2),\n",
    "            data_root=confs[\"data-root\"],\n",
    "            percent=confs[\"nn-mem-percent\"],\n",
    "        ),\n",
    "        batch_size=confs[\"batch-size\"], shuffle=False,\n",
    "        num_workers=confs[\"workers\"]//2, pin_memory=True,\n",
    "        persistent_workers=True)\n",
    "\n",
    "    if confs[\"evaluate\"]:\n",
    "        ss_validate(val_loader_base, val_loader_query, model, confs)\n",
    "        return\n",
    "\n",
    "    best_epoch = confs[\"start-epoch\"]\n",
    "    print('Start the training')\n",
    "    epoch = 0\n",
    "    for epoch in range(confs[\"start-epoch\"], confs[\"epochs\"]):\n",
    "        if epoch >= confs[\"warmup-epoch\"]:\n",
    "            lr_schedule.adjust_learning_rate(optimizer, epoch, confs)\n",
    "\n",
    "        # train for one epoch\n",
    "        train(train_loader, model, optimizer, scaler, epoch, confs)\n",
    "        if (epoch + 1) % confs[\"eval_freq\"] == 0:\n",
    "            val_loss = validate(val_loader_base , model,confs)\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_epoch = epoch\n",
    "        \n",
    "        if not confs[\"multiprocessing-distributed\"] or (confs[\"multiprocessing-distributed\"] and \n",
    "        confs[\"local_rank\"] % confs[\"world-size\"] == 0):\n",
    "            utils.save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': confs[\"arch\"],\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_acc1': best_acc1,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scaler': None if scaler is None else scaler.state_dict(),\n",
    "        }, epoch=epoch, confs=confs)\n",
    "\n",
    "    print('Best Acc@1 {0} @ epoch {1}'.format(best_acc1, best_epoch + 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, optimizer, scaler, epoch, args):\n",
    "    batch_time = utils.AverageMeter('Time', ':6.3f')\n",
    "    data_time = utils.AverageMeter('Data', ':6.3f')\n",
    "    losses = utils.AverageMeter('Loss', ':.4e')\n",
    "    curr_lr = utils.InstantMeter('LR', '')\n",
    "    curr_mom = utils.InstantMeter('MOM', '')\n",
    "    progress = utils.ProgressMeter(\n",
    "        len(train_loader),\n",
    "        [curr_lr, curr_mom, batch_time, data_time, losses],\n",
    "        prefix=\"Epoch: [{}/{}]\\t\".format(epoch, confs[\"epochs\"]))\n",
    "\n",
    "    # iter info\n",
    "    batch_iter = len(train_loader)\n",
    "    max_iter = float(batch_iter * confs[\"epochs\"])\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    if \"EMAN\" in confs[\"arch\"]:\n",
    "        print(\"setting the key model to eval mode when using EMAN\")\n",
    "        if hasattr(model, 'module'):\n",
    "            model.module.target_net.eval()\n",
    "        else:\n",
    "            model.target_net.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (images, caption, idx) in enumerate(train_loader):\n",
    "        # update model momentum\n",
    "        curr_iter = float(epoch * batch_iter + i)\n",
    "\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if confs[\"gpu\"] is not None:\n",
    "            images[0] = images[0].cuda(confs[\"gpu\"], non_blocking=True)\n",
    "            images[1] = images[1].cuda(confs[\"gpu\"], non_blocking=True)\n",
    "            idx = idx.cuda(confs[\"gpu\"], non_blocking=True)\n",
    "\n",
    "        # warmup learning rate\n",
    "        if epoch < confs[\"warmup-epoch\"]:\n",
    "            warmup_step = confs[\"warmup-epoch\"] * batch_iter\n",
    "            curr_step = epoch * batch_iter + i + 1\n",
    "            lr_schedule.warmup_learning_rate(optimizer, curr_step, warmup_step, confs)\n",
    "        curr_lr.update(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        if scaler is None:\n",
    "            # compute loss\n",
    "            with torch.cuda.amp.autocast():\n",
    "                loss = model(im_v1=images[0], im_v2=images[1] , caption = caption, idx=idx)\n",
    "            # measure accuracy and record loss\n",
    "            losses.update(loss.item(), images[0].size(0))\n",
    "\n",
    "            # compute gradient and do SGD step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:   # AMP\n",
    "            optimizer.zero_grad()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                loss = model(im_v1=images[0], im_v2=images[1], idx=idx)\n",
    "            # measure accuracy and record loss\n",
    "            losses.update(loss.item(), images[0].size(0))\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        \n",
    "        if hasattr(model, 'module'):\n",
    "            model.module.momentum_update(curr_iter, max_iter)\n",
    "            curr_mom.update(model.module.curr_m)\n",
    "        else:\n",
    "            model.momentum_update(curr_iter, max_iter)\n",
    "            curr_mom.update(model.curr_m)\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % confs[\"print_freq\"] == 0:\n",
    "            progress.display(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_params(model, exclude_bias_and_bn=True, sync_bn=True):\n",
    "    \"\"\"\n",
    "    exclude_bias_and bn: exclude bias and bn from both weight decay and LARS adaptation\n",
    "        in the PyTorch implementation of ResNet, `downsample.1` are bn layers\n",
    "    \"\"\"\n",
    "    weight_param_list, bn_and_bias_param_list = [], []\n",
    "    weight_param_names, bn_and_bias_param_names = [], []\n",
    "    for name, param in model.named_parameters():\n",
    "        if exclude_bias_and_bn and ('bn' in name or 'downsample.1' in name or 'bias' in name or (sync_bn and 'mlp.1' in name)):\n",
    "            bn_and_bias_param_list.append(param)\n",
    "            bn_and_bias_param_names.append(name)\n",
    "        else:\n",
    "            weight_param_list.append(param)\n",
    "            weight_param_names.append(name)\n",
    "    print(\"weight params:\\n{}\".format('\\n'.join(weight_param_names)))\n",
    "    print(\"bn and bias params:\\n{}\".format('\\n'.join(bn_and_bias_param_names)))\n",
    "    param_list = [{'params': bn_and_bias_param_list, 'weight_decay': 0., 'lars_exclude': True},\n",
    "                  {'params': weight_param_list}]\n",
    "    return param_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> creating model 'LEWELB' with backbone 'clip_encoder'\n",
      "LEWELB(\n",
      "  loss_weight=0.5\n",
      "  (online_net): EncoderObj(\n",
      "    (backbone): CLIP(\n",
      "      (model): CLIP(\n",
      "        (visual): ModifiedResNet(\n",
      "          (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu3): ReLU(inplace=True)\n",
      "          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "          (layer1): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu2): ReLU(inplace=True)\n",
      "              (avgpool): Identity()\n",
      "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu3): ReLU(inplace=True)\n",
      "              (downsample): Sequential(\n",
      "                (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu2): ReLU(inplace=True)\n",
      "              (avgpool): Identity()\n",
      "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu3): ReLU(inplace=True)\n",
      "            )\n",
      "            (2): Bottleneck(\n",
      "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu2): ReLU(inplace=True)\n",
      "              (avgpool): Identity()\n",
      "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu3): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (layer2): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu2): ReLU(inplace=True)\n",
      "              (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu3): ReLU(inplace=True)\n",
      "              (downsample): Sequential(\n",
      "                (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu2): ReLU(inplace=True)\n",
      "              (avgpool): Identity()\n",
      "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu3): ReLU(inplace=True)\n",
      "            )\n",
      "            (2): Bottleneck(\n",
      "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu2): ReLU(inplace=True)\n",
      "              (avgpool): Identity()\n",
      "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu3): ReLU(inplace=True)\n",
      "            )\n",
      "            (3): Bottleneck(\n",
      "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu2): ReLU(inplace=True)\n",
      "              (avgpool): Identity()\n",
      "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu3): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (layer3): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu2): ReLU(inplace=True)\n",
      "              (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu3): ReLU(inplace=True)\n",
      "              (downsample): Sequential(\n",
      "                (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "                (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu2): ReLU(inplace=True)\n",
      "              (avgpool): Identity()\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu3): ReLU(inplace=True)\n",
      "            )\n",
      "            (2): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu2): ReLU(inplace=True)\n",
      "              (avgpool): Identity()\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu3): ReLU(inplace=True)\n",
      "            )\n",
      "            (3): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu2): ReLU(inplace=True)\n",
      "              (avgpool): Identity()\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu3): ReLU(inplace=True)\n",
      "            )\n",
      "            (4): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu2): ReLU(inplace=True)\n",
      "              (avgpool): Identity()\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu3): ReLU(inplace=True)\n",
      "            )\n",
      "            (5): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu2): ReLU(inplace=True)\n",
      "              (avgpool): Identity()\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu3): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (layer4): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu2): ReLU(inplace=True)\n",
      "              (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu3): ReLU(inplace=True)\n",
      "              (downsample): Sequential(\n",
      "                (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "                (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu2): ReLU(inplace=True)\n",
      "              (avgpool): Identity()\n",
      "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu3): ReLU(inplace=True)\n",
      "            )\n",
      "            (2): Bottleneck(\n",
      "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu2): ReLU(inplace=True)\n",
      "              (avgpool): Identity()\n",
      "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu3): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (attnpool): Identity()\n",
      "        )\n",
      "        (transformer): Transformer(\n",
      "          (resblocks): Sequential(\n",
      "            (0): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (2): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (3): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (4): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (5): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (6): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (7): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (8): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (9): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (10): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (11): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (token_embedding): Embedding(49408, 512)\n",
      "        (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (feed_forward): FeedForwardNeural(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "          (3): ReLU()\n",
      "          (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "          (5): ReLU()\n",
      "          (6): Linear(in_features=4096, out_features=49, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (neck): ObjectNeck(\n",
      "      scale=1, l2_norm=True, num_heads=4\n",
      "      (proj): MLP1D(\n",
      "        (mlp): Sequential(\n",
      "          (0): Conv1d(2048, 4096, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv1d(4096, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (proj_obj): MLP1D(\n",
      "        (mlp): Sequential(\n",
      "          (0): Conv1d(2048, 4096, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv1d(4096, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (target_net): EncoderObj(\n",
      "    (backbone): CLIP(\n",
      "      (model): CLIP(\n",
      "        (visual): ModifiedResNet(\n",
      "          (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu1): ReLU(inplace=True)\n",
      "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu2): ReLU(inplace=True)\n",
      "          (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu3): ReLU(inplace=True)\n",
      "          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "          (layer1): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu2): ReLU(inplace=True)\n",
      "              (avgpool): Identity()\n",
      "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu3): ReLU(inplace=True)\n",
      "              (downsample): Sequential(\n",
      "                (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "                (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu2): ReLU(inplace=True)\n",
      "              (avgpool): Identity()\n",
      "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu3): ReLU(inplace=True)\n",
      "            )\n",
      "            (2): Bottleneck(\n",
      "              (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu2): ReLU(inplace=True)\n",
      "              (avgpool): Identity()\n",
      "              (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu3): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (layer2): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu2): ReLU(inplace=True)\n",
      "              (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu3): ReLU(inplace=True)\n",
      "              (downsample): Sequential(\n",
      "                (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu2): ReLU(inplace=True)\n",
      "              (avgpool): Identity()\n",
      "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu3): ReLU(inplace=True)\n",
      "            )\n",
      "            (2): Bottleneck(\n",
      "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu2): ReLU(inplace=True)\n",
      "              (avgpool): Identity()\n",
      "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu3): ReLU(inplace=True)\n",
      "            )\n",
      "            (3): Bottleneck(\n",
      "              (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu2): ReLU(inplace=True)\n",
      "              (avgpool): Identity()\n",
      "              (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu3): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (layer3): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu2): ReLU(inplace=True)\n",
      "              (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu3): ReLU(inplace=True)\n",
      "              (downsample): Sequential(\n",
      "                (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "                (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu2): ReLU(inplace=True)\n",
      "              (avgpool): Identity()\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu3): ReLU(inplace=True)\n",
      "            )\n",
      "            (2): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu2): ReLU(inplace=True)\n",
      "              (avgpool): Identity()\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu3): ReLU(inplace=True)\n",
      "            )\n",
      "            (3): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu2): ReLU(inplace=True)\n",
      "              (avgpool): Identity()\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu3): ReLU(inplace=True)\n",
      "            )\n",
      "            (4): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu2): ReLU(inplace=True)\n",
      "              (avgpool): Identity()\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu3): ReLU(inplace=True)\n",
      "            )\n",
      "            (5): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu2): ReLU(inplace=True)\n",
      "              (avgpool): Identity()\n",
      "              (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu3): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (layer4): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu2): ReLU(inplace=True)\n",
      "              (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu3): ReLU(inplace=True)\n",
      "              (downsample): Sequential(\n",
      "                (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "                (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu2): ReLU(inplace=True)\n",
      "              (avgpool): Identity()\n",
      "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu3): ReLU(inplace=True)\n",
      "            )\n",
      "            (2): Bottleneck(\n",
      "              (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu1): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu2): ReLU(inplace=True)\n",
      "              (avgpool): Identity()\n",
      "              (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu3): ReLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (attnpool): Identity()\n",
      "        )\n",
      "        (transformer): Transformer(\n",
      "          (resblocks): Sequential(\n",
      "            (0): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (2): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (3): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (4): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (5): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (6): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (7): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (8): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (9): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (10): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (11): ResidualAttentionBlock(\n",
      "              (attn): MultiheadAttention(\n",
      "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Sequential(\n",
      "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (gelu): QuickGELU()\n",
      "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              )\n",
      "              (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (token_embedding): Embedding(49408, 512)\n",
      "        (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (feed_forward): FeedForwardNeural(\n",
      "        (mlp): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "          (3): ReLU()\n",
      "          (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "          (5): ReLU()\n",
      "          (6): Linear(in_features=4096, out_features=49, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (neck): ObjectNeck(\n",
      "      scale=1, l2_norm=True, num_heads=4\n",
      "      (proj): MLP1D(\n",
      "        (mlp): Sequential(\n",
      "          (0): Conv1d(2048, 4096, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv1d(4096, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (proj_obj): MLP1D(\n",
      "        (mlp): Sequential(\n",
      "          (0): Conv1d(2048, 4096, kernel_size=(1,), stride=(1,), bias=False)\n",
      "          (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv1d(4096, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (predictor): MLP1D(\n",
      "    (mlp): Sequential(\n",
      "      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv1d(4096, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (predictor_obj): MLP1D(\n",
      "    (mlp): Sequential(\n",
      "      (0): Conv1d(256, 4096, kernel_size=(1,), stride=(1,), bias=False)\n",
      "      (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv1d(4096, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "{'local_rank': 0, 'dataset': 'newsclippings', 'data-root': 'E:/Chinmay/news_clippings', 'arch': 'LEWELB', 'backbone': 'clip_encoder', 'workers': 8, 'epochs': 150, 'start-epoch': 0, 'warmup-epoch': 10, 'batch-size': 16, 'learning_rate': 0.001, 'schedule': [120, 160], 'cos': True, 'momentum': 0.9, 'weight_decay': 1e-05, 'save-dir': '/projects/academic/sreyasee/chinmayd/saved_models', 'print_freq': 200, 'save_freq': 10, 'eval_freq': 1, 'resume': None, 'pretrained': '', 'super-pretrained': '', 'evaluate': False, 'world-size': 1, 'rank': 0, 'dist-url': 'tcp://224.66.41.62:23456', 'dist-backend': 'nccl', 'seed': '23456', 'gpu': None, 'port': 5389, 'multiprocessing-distributed': None, 'proj_dim': 256, 'enc-m': 0.996, 'norm': None, 'num-neck-mlp': 2, 'hid-dim': 4096, 'amp': None, 'lewel-l2-norm': True, 'lewel-scale': 1, 'lewel-num-heads': 4, 'lewel-loss-weight': 0.5, 'num-nn': 20, 'nn-mem-percent': 0.1, 'nn-query-percent': 0.5, 'scale': 1, 'distributed': False, 'multiprocessing_distributed': False}\n",
      "weight params:\n",
      "online_net.backbone.model.positional_embedding\n",
      "online_net.backbone.model.text_projection\n",
      "online_net.backbone.model.logit_scale\n",
      "online_net.backbone.model.visual.conv1.weight\n",
      "online_net.backbone.model.visual.conv2.weight\n",
      "online_net.backbone.model.visual.conv3.weight\n",
      "online_net.backbone.model.visual.layer1.0.conv1.weight\n",
      "online_net.backbone.model.visual.layer1.0.conv2.weight\n",
      "online_net.backbone.model.visual.layer1.0.conv3.weight\n",
      "online_net.backbone.model.visual.layer1.0.downsample.0.weight\n",
      "online_net.backbone.model.visual.layer1.1.conv1.weight\n",
      "online_net.backbone.model.visual.layer1.1.conv2.weight\n",
      "online_net.backbone.model.visual.layer1.1.conv3.weight\n",
      "online_net.backbone.model.visual.layer1.2.conv1.weight\n",
      "online_net.backbone.model.visual.layer1.2.conv2.weight\n",
      "online_net.backbone.model.visual.layer1.2.conv3.weight\n",
      "online_net.backbone.model.visual.layer2.0.conv1.weight\n",
      "online_net.backbone.model.visual.layer2.0.conv2.weight\n",
      "online_net.backbone.model.visual.layer2.0.conv3.weight\n",
      "online_net.backbone.model.visual.layer2.0.downsample.0.weight\n",
      "online_net.backbone.model.visual.layer2.1.conv1.weight\n",
      "online_net.backbone.model.visual.layer2.1.conv2.weight\n",
      "online_net.backbone.model.visual.layer2.1.conv3.weight\n",
      "online_net.backbone.model.visual.layer2.2.conv1.weight\n",
      "online_net.backbone.model.visual.layer2.2.conv2.weight\n",
      "online_net.backbone.model.visual.layer2.2.conv3.weight\n",
      "online_net.backbone.model.visual.layer2.3.conv1.weight\n",
      "online_net.backbone.model.visual.layer2.3.conv2.weight\n",
      "online_net.backbone.model.visual.layer2.3.conv3.weight\n",
      "online_net.backbone.model.visual.layer3.0.conv1.weight\n",
      "online_net.backbone.model.visual.layer3.0.conv2.weight\n",
      "online_net.backbone.model.visual.layer3.0.conv3.weight\n",
      "online_net.backbone.model.visual.layer3.0.downsample.0.weight\n",
      "online_net.backbone.model.visual.layer3.1.conv1.weight\n",
      "online_net.backbone.model.visual.layer3.1.conv2.weight\n",
      "online_net.backbone.model.visual.layer3.1.conv3.weight\n",
      "online_net.backbone.model.visual.layer3.2.conv1.weight\n",
      "online_net.backbone.model.visual.layer3.2.conv2.weight\n",
      "online_net.backbone.model.visual.layer3.2.conv3.weight\n",
      "online_net.backbone.model.visual.layer3.3.conv1.weight\n",
      "online_net.backbone.model.visual.layer3.3.conv2.weight\n",
      "online_net.backbone.model.visual.layer3.3.conv3.weight\n",
      "online_net.backbone.model.visual.layer3.4.conv1.weight\n",
      "online_net.backbone.model.visual.layer3.4.conv2.weight\n",
      "online_net.backbone.model.visual.layer3.4.conv3.weight\n",
      "online_net.backbone.model.visual.layer3.5.conv1.weight\n",
      "online_net.backbone.model.visual.layer3.5.conv2.weight\n",
      "online_net.backbone.model.visual.layer3.5.conv3.weight\n",
      "online_net.backbone.model.visual.layer4.0.conv1.weight\n",
      "online_net.backbone.model.visual.layer4.0.conv2.weight\n",
      "online_net.backbone.model.visual.layer4.0.conv3.weight\n",
      "online_net.backbone.model.visual.layer4.0.downsample.0.weight\n",
      "online_net.backbone.model.visual.layer4.1.conv1.weight\n",
      "online_net.backbone.model.visual.layer4.1.conv2.weight\n",
      "online_net.backbone.model.visual.layer4.1.conv3.weight\n",
      "online_net.backbone.model.visual.layer4.2.conv1.weight\n",
      "online_net.backbone.model.visual.layer4.2.conv2.weight\n",
      "online_net.backbone.model.visual.layer4.2.conv3.weight\n",
      "online_net.backbone.model.transformer.resblocks.0.attn.in_proj_weight\n",
      "online_net.backbone.model.transformer.resblocks.0.attn.out_proj.weight\n",
      "online_net.backbone.model.transformer.resblocks.0.ln_1.weight\n",
      "online_net.backbone.model.transformer.resblocks.0.mlp.c_fc.weight\n",
      "online_net.backbone.model.transformer.resblocks.0.mlp.c_proj.weight\n",
      "online_net.backbone.model.transformer.resblocks.0.ln_2.weight\n",
      "online_net.backbone.model.transformer.resblocks.1.attn.in_proj_weight\n",
      "online_net.backbone.model.transformer.resblocks.1.attn.out_proj.weight\n",
      "online_net.backbone.model.transformer.resblocks.1.ln_1.weight\n",
      "online_net.backbone.model.transformer.resblocks.1.mlp.c_fc.weight\n",
      "online_net.backbone.model.transformer.resblocks.1.mlp.c_proj.weight\n",
      "online_net.backbone.model.transformer.resblocks.1.ln_2.weight\n",
      "online_net.backbone.model.transformer.resblocks.2.attn.in_proj_weight\n",
      "online_net.backbone.model.transformer.resblocks.2.attn.out_proj.weight\n",
      "online_net.backbone.model.transformer.resblocks.2.ln_1.weight\n",
      "online_net.backbone.model.transformer.resblocks.2.mlp.c_fc.weight\n",
      "online_net.backbone.model.transformer.resblocks.2.mlp.c_proj.weight\n",
      "online_net.backbone.model.transformer.resblocks.2.ln_2.weight\n",
      "online_net.backbone.model.transformer.resblocks.3.attn.in_proj_weight\n",
      "online_net.backbone.model.transformer.resblocks.3.attn.out_proj.weight\n",
      "online_net.backbone.model.transformer.resblocks.3.ln_1.weight\n",
      "online_net.backbone.model.transformer.resblocks.3.mlp.c_fc.weight\n",
      "online_net.backbone.model.transformer.resblocks.3.mlp.c_proj.weight\n",
      "online_net.backbone.model.transformer.resblocks.3.ln_2.weight\n",
      "online_net.backbone.model.transformer.resblocks.4.attn.in_proj_weight\n",
      "online_net.backbone.model.transformer.resblocks.4.attn.out_proj.weight\n",
      "online_net.backbone.model.transformer.resblocks.4.ln_1.weight\n",
      "online_net.backbone.model.transformer.resblocks.4.mlp.c_fc.weight\n",
      "online_net.backbone.model.transformer.resblocks.4.mlp.c_proj.weight\n",
      "online_net.backbone.model.transformer.resblocks.4.ln_2.weight\n",
      "online_net.backbone.model.transformer.resblocks.5.attn.in_proj_weight\n",
      "online_net.backbone.model.transformer.resblocks.5.attn.out_proj.weight\n",
      "online_net.backbone.model.transformer.resblocks.5.ln_1.weight\n",
      "online_net.backbone.model.transformer.resblocks.5.mlp.c_fc.weight\n",
      "online_net.backbone.model.transformer.resblocks.5.mlp.c_proj.weight\n",
      "online_net.backbone.model.transformer.resblocks.5.ln_2.weight\n",
      "online_net.backbone.model.transformer.resblocks.6.attn.in_proj_weight\n",
      "online_net.backbone.model.transformer.resblocks.6.attn.out_proj.weight\n",
      "online_net.backbone.model.transformer.resblocks.6.ln_1.weight\n",
      "online_net.backbone.model.transformer.resblocks.6.mlp.c_fc.weight\n",
      "online_net.backbone.model.transformer.resblocks.6.mlp.c_proj.weight\n",
      "online_net.backbone.model.transformer.resblocks.6.ln_2.weight\n",
      "online_net.backbone.model.transformer.resblocks.7.attn.in_proj_weight\n",
      "online_net.backbone.model.transformer.resblocks.7.attn.out_proj.weight\n",
      "online_net.backbone.model.transformer.resblocks.7.ln_1.weight\n",
      "online_net.backbone.model.transformer.resblocks.7.mlp.c_fc.weight\n",
      "online_net.backbone.model.transformer.resblocks.7.mlp.c_proj.weight\n",
      "online_net.backbone.model.transformer.resblocks.7.ln_2.weight\n",
      "online_net.backbone.model.transformer.resblocks.8.attn.in_proj_weight\n",
      "online_net.backbone.model.transformer.resblocks.8.attn.out_proj.weight\n",
      "online_net.backbone.model.transformer.resblocks.8.ln_1.weight\n",
      "online_net.backbone.model.transformer.resblocks.8.mlp.c_fc.weight\n",
      "online_net.backbone.model.transformer.resblocks.8.mlp.c_proj.weight\n",
      "online_net.backbone.model.transformer.resblocks.8.ln_2.weight\n",
      "online_net.backbone.model.transformer.resblocks.9.attn.in_proj_weight\n",
      "online_net.backbone.model.transformer.resblocks.9.attn.out_proj.weight\n",
      "online_net.backbone.model.transformer.resblocks.9.ln_1.weight\n",
      "online_net.backbone.model.transformer.resblocks.9.mlp.c_fc.weight\n",
      "online_net.backbone.model.transformer.resblocks.9.mlp.c_proj.weight\n",
      "online_net.backbone.model.transformer.resblocks.9.ln_2.weight\n",
      "online_net.backbone.model.transformer.resblocks.10.attn.in_proj_weight\n",
      "online_net.backbone.model.transformer.resblocks.10.attn.out_proj.weight\n",
      "online_net.backbone.model.transformer.resblocks.10.ln_1.weight\n",
      "online_net.backbone.model.transformer.resblocks.10.mlp.c_fc.weight\n",
      "online_net.backbone.model.transformer.resblocks.10.mlp.c_proj.weight\n",
      "online_net.backbone.model.transformer.resblocks.10.ln_2.weight\n",
      "online_net.backbone.model.transformer.resblocks.11.attn.in_proj_weight\n",
      "online_net.backbone.model.transformer.resblocks.11.attn.out_proj.weight\n",
      "online_net.backbone.model.transformer.resblocks.11.ln_1.weight\n",
      "online_net.backbone.model.transformer.resblocks.11.mlp.c_fc.weight\n",
      "online_net.backbone.model.transformer.resblocks.11.mlp.c_proj.weight\n",
      "online_net.backbone.model.transformer.resblocks.11.ln_2.weight\n",
      "online_net.backbone.model.token_embedding.weight\n",
      "online_net.backbone.model.ln_final.weight\n",
      "online_net.backbone.feed_forward.mlp.0.weight\n",
      "online_net.backbone.feed_forward.mlp.2.weight\n",
      "online_net.backbone.feed_forward.mlp.4.weight\n",
      "online_net.backbone.feed_forward.mlp.6.weight\n",
      "online_net.neck.proj.mlp.0.weight\n",
      "online_net.neck.proj.mlp.1.weight\n",
      "online_net.neck.proj.mlp.3.weight\n",
      "online_net.neck.proj_obj.mlp.0.weight\n",
      "online_net.neck.proj_obj.mlp.1.weight\n",
      "online_net.neck.proj_obj.mlp.3.weight\n",
      "target_net.backbone.model.positional_embedding\n",
      "target_net.backbone.model.text_projection\n",
      "target_net.backbone.model.logit_scale\n",
      "target_net.backbone.model.visual.conv1.weight\n",
      "target_net.backbone.model.visual.conv2.weight\n",
      "target_net.backbone.model.visual.conv3.weight\n",
      "target_net.backbone.model.visual.layer1.0.conv1.weight\n",
      "target_net.backbone.model.visual.layer1.0.conv2.weight\n",
      "target_net.backbone.model.visual.layer1.0.conv3.weight\n",
      "target_net.backbone.model.visual.layer1.0.downsample.0.weight\n",
      "target_net.backbone.model.visual.layer1.1.conv1.weight\n",
      "target_net.backbone.model.visual.layer1.1.conv2.weight\n",
      "target_net.backbone.model.visual.layer1.1.conv3.weight\n",
      "target_net.backbone.model.visual.layer1.2.conv1.weight\n",
      "target_net.backbone.model.visual.layer1.2.conv2.weight\n",
      "target_net.backbone.model.visual.layer1.2.conv3.weight\n",
      "target_net.backbone.model.visual.layer2.0.conv1.weight\n",
      "target_net.backbone.model.visual.layer2.0.conv2.weight\n",
      "target_net.backbone.model.visual.layer2.0.conv3.weight\n",
      "target_net.backbone.model.visual.layer2.0.downsample.0.weight\n",
      "target_net.backbone.model.visual.layer2.1.conv1.weight\n",
      "target_net.backbone.model.visual.layer2.1.conv2.weight\n",
      "target_net.backbone.model.visual.layer2.1.conv3.weight\n",
      "target_net.backbone.model.visual.layer2.2.conv1.weight\n",
      "target_net.backbone.model.visual.layer2.2.conv2.weight\n",
      "target_net.backbone.model.visual.layer2.2.conv3.weight\n",
      "target_net.backbone.model.visual.layer2.3.conv1.weight\n",
      "target_net.backbone.model.visual.layer2.3.conv2.weight\n",
      "target_net.backbone.model.visual.layer2.3.conv3.weight\n",
      "target_net.backbone.model.visual.layer3.0.conv1.weight\n",
      "target_net.backbone.model.visual.layer3.0.conv2.weight\n",
      "target_net.backbone.model.visual.layer3.0.conv3.weight\n",
      "target_net.backbone.model.visual.layer3.0.downsample.0.weight\n",
      "target_net.backbone.model.visual.layer3.1.conv1.weight\n",
      "target_net.backbone.model.visual.layer3.1.conv2.weight\n",
      "target_net.backbone.model.visual.layer3.1.conv3.weight\n",
      "target_net.backbone.model.visual.layer3.2.conv1.weight\n",
      "target_net.backbone.model.visual.layer3.2.conv2.weight\n",
      "target_net.backbone.model.visual.layer3.2.conv3.weight\n",
      "target_net.backbone.model.visual.layer3.3.conv1.weight\n",
      "target_net.backbone.model.visual.layer3.3.conv2.weight\n",
      "target_net.backbone.model.visual.layer3.3.conv3.weight\n",
      "target_net.backbone.model.visual.layer3.4.conv1.weight\n",
      "target_net.backbone.model.visual.layer3.4.conv2.weight\n",
      "target_net.backbone.model.visual.layer3.4.conv3.weight\n",
      "target_net.backbone.model.visual.layer3.5.conv1.weight\n",
      "target_net.backbone.model.visual.layer3.5.conv2.weight\n",
      "target_net.backbone.model.visual.layer3.5.conv3.weight\n",
      "target_net.backbone.model.visual.layer4.0.conv1.weight\n",
      "target_net.backbone.model.visual.layer4.0.conv2.weight\n",
      "target_net.backbone.model.visual.layer4.0.conv3.weight\n",
      "target_net.backbone.model.visual.layer4.0.downsample.0.weight\n",
      "target_net.backbone.model.visual.layer4.1.conv1.weight\n",
      "target_net.backbone.model.visual.layer4.1.conv2.weight\n",
      "target_net.backbone.model.visual.layer4.1.conv3.weight\n",
      "target_net.backbone.model.visual.layer4.2.conv1.weight\n",
      "target_net.backbone.model.visual.layer4.2.conv2.weight\n",
      "target_net.backbone.model.visual.layer4.2.conv3.weight\n",
      "target_net.backbone.model.transformer.resblocks.0.attn.in_proj_weight\n",
      "target_net.backbone.model.transformer.resblocks.0.attn.out_proj.weight\n",
      "target_net.backbone.model.transformer.resblocks.0.ln_1.weight\n",
      "target_net.backbone.model.transformer.resblocks.0.mlp.c_fc.weight\n",
      "target_net.backbone.model.transformer.resblocks.0.mlp.c_proj.weight\n",
      "target_net.backbone.model.transformer.resblocks.0.ln_2.weight\n",
      "target_net.backbone.model.transformer.resblocks.1.attn.in_proj_weight\n",
      "target_net.backbone.model.transformer.resblocks.1.attn.out_proj.weight\n",
      "target_net.backbone.model.transformer.resblocks.1.ln_1.weight\n",
      "target_net.backbone.model.transformer.resblocks.1.mlp.c_fc.weight\n",
      "target_net.backbone.model.transformer.resblocks.1.mlp.c_proj.weight\n",
      "target_net.backbone.model.transformer.resblocks.1.ln_2.weight\n",
      "target_net.backbone.model.transformer.resblocks.2.attn.in_proj_weight\n",
      "target_net.backbone.model.transformer.resblocks.2.attn.out_proj.weight\n",
      "target_net.backbone.model.transformer.resblocks.2.ln_1.weight\n",
      "target_net.backbone.model.transformer.resblocks.2.mlp.c_fc.weight\n",
      "target_net.backbone.model.transformer.resblocks.2.mlp.c_proj.weight\n",
      "target_net.backbone.model.transformer.resblocks.2.ln_2.weight\n",
      "target_net.backbone.model.transformer.resblocks.3.attn.in_proj_weight\n",
      "target_net.backbone.model.transformer.resblocks.3.attn.out_proj.weight\n",
      "target_net.backbone.model.transformer.resblocks.3.ln_1.weight\n",
      "target_net.backbone.model.transformer.resblocks.3.mlp.c_fc.weight\n",
      "target_net.backbone.model.transformer.resblocks.3.mlp.c_proj.weight\n",
      "target_net.backbone.model.transformer.resblocks.3.ln_2.weight\n",
      "target_net.backbone.model.transformer.resblocks.4.attn.in_proj_weight\n",
      "target_net.backbone.model.transformer.resblocks.4.attn.out_proj.weight\n",
      "target_net.backbone.model.transformer.resblocks.4.ln_1.weight\n",
      "target_net.backbone.model.transformer.resblocks.4.mlp.c_fc.weight\n",
      "target_net.backbone.model.transformer.resblocks.4.mlp.c_proj.weight\n",
      "target_net.backbone.model.transformer.resblocks.4.ln_2.weight\n",
      "target_net.backbone.model.transformer.resblocks.5.attn.in_proj_weight\n",
      "target_net.backbone.model.transformer.resblocks.5.attn.out_proj.weight\n",
      "target_net.backbone.model.transformer.resblocks.5.ln_1.weight\n",
      "target_net.backbone.model.transformer.resblocks.5.mlp.c_fc.weight\n",
      "target_net.backbone.model.transformer.resblocks.5.mlp.c_proj.weight\n",
      "target_net.backbone.model.transformer.resblocks.5.ln_2.weight\n",
      "target_net.backbone.model.transformer.resblocks.6.attn.in_proj_weight\n",
      "target_net.backbone.model.transformer.resblocks.6.attn.out_proj.weight\n",
      "target_net.backbone.model.transformer.resblocks.6.ln_1.weight\n",
      "target_net.backbone.model.transformer.resblocks.6.mlp.c_fc.weight\n",
      "target_net.backbone.model.transformer.resblocks.6.mlp.c_proj.weight\n",
      "target_net.backbone.model.transformer.resblocks.6.ln_2.weight\n",
      "target_net.backbone.model.transformer.resblocks.7.attn.in_proj_weight\n",
      "target_net.backbone.model.transformer.resblocks.7.attn.out_proj.weight\n",
      "target_net.backbone.model.transformer.resblocks.7.ln_1.weight\n",
      "target_net.backbone.model.transformer.resblocks.7.mlp.c_fc.weight\n",
      "target_net.backbone.model.transformer.resblocks.7.mlp.c_proj.weight\n",
      "target_net.backbone.model.transformer.resblocks.7.ln_2.weight\n",
      "target_net.backbone.model.transformer.resblocks.8.attn.in_proj_weight\n",
      "target_net.backbone.model.transformer.resblocks.8.attn.out_proj.weight\n",
      "target_net.backbone.model.transformer.resblocks.8.ln_1.weight\n",
      "target_net.backbone.model.transformer.resblocks.8.mlp.c_fc.weight\n",
      "target_net.backbone.model.transformer.resblocks.8.mlp.c_proj.weight\n",
      "target_net.backbone.model.transformer.resblocks.8.ln_2.weight\n",
      "target_net.backbone.model.transformer.resblocks.9.attn.in_proj_weight\n",
      "target_net.backbone.model.transformer.resblocks.9.attn.out_proj.weight\n",
      "target_net.backbone.model.transformer.resblocks.9.ln_1.weight\n",
      "target_net.backbone.model.transformer.resblocks.9.mlp.c_fc.weight\n",
      "target_net.backbone.model.transformer.resblocks.9.mlp.c_proj.weight\n",
      "target_net.backbone.model.transformer.resblocks.9.ln_2.weight\n",
      "target_net.backbone.model.transformer.resblocks.10.attn.in_proj_weight\n",
      "target_net.backbone.model.transformer.resblocks.10.attn.out_proj.weight\n",
      "target_net.backbone.model.transformer.resblocks.10.ln_1.weight\n",
      "target_net.backbone.model.transformer.resblocks.10.mlp.c_fc.weight\n",
      "target_net.backbone.model.transformer.resblocks.10.mlp.c_proj.weight\n",
      "target_net.backbone.model.transformer.resblocks.10.ln_2.weight\n",
      "target_net.backbone.model.transformer.resblocks.11.attn.in_proj_weight\n",
      "target_net.backbone.model.transformer.resblocks.11.attn.out_proj.weight\n",
      "target_net.backbone.model.transformer.resblocks.11.ln_1.weight\n",
      "target_net.backbone.model.transformer.resblocks.11.mlp.c_fc.weight\n",
      "target_net.backbone.model.transformer.resblocks.11.mlp.c_proj.weight\n",
      "target_net.backbone.model.transformer.resblocks.11.ln_2.weight\n",
      "target_net.backbone.model.token_embedding.weight\n",
      "target_net.backbone.model.ln_final.weight\n",
      "target_net.backbone.feed_forward.mlp.0.weight\n",
      "target_net.backbone.feed_forward.mlp.2.weight\n",
      "target_net.backbone.feed_forward.mlp.4.weight\n",
      "target_net.backbone.feed_forward.mlp.6.weight\n",
      "target_net.neck.proj.mlp.0.weight\n",
      "target_net.neck.proj.mlp.1.weight\n",
      "target_net.neck.proj.mlp.3.weight\n",
      "target_net.neck.proj_obj.mlp.0.weight\n",
      "target_net.neck.proj_obj.mlp.1.weight\n",
      "target_net.neck.proj_obj.mlp.3.weight\n",
      "predictor.mlp.0.weight\n",
      "predictor.mlp.1.weight\n",
      "predictor.mlp.3.weight\n",
      "predictor_obj.mlp.0.weight\n",
      "predictor_obj.mlp.1.weight\n",
      "predictor_obj.mlp.3.weight\n",
      "bn and bias params:\n",
      "online_net.backbone.model.visual.bn1.weight\n",
      "online_net.backbone.model.visual.bn1.bias\n",
      "online_net.backbone.model.visual.bn2.weight\n",
      "online_net.backbone.model.visual.bn2.bias\n",
      "online_net.backbone.model.visual.bn3.weight\n",
      "online_net.backbone.model.visual.bn3.bias\n",
      "online_net.backbone.model.visual.layer1.0.bn1.weight\n",
      "online_net.backbone.model.visual.layer1.0.bn1.bias\n",
      "online_net.backbone.model.visual.layer1.0.bn2.weight\n",
      "online_net.backbone.model.visual.layer1.0.bn2.bias\n",
      "online_net.backbone.model.visual.layer1.0.bn3.weight\n",
      "online_net.backbone.model.visual.layer1.0.bn3.bias\n",
      "online_net.backbone.model.visual.layer1.0.downsample.1.weight\n",
      "online_net.backbone.model.visual.layer1.0.downsample.1.bias\n",
      "online_net.backbone.model.visual.layer1.1.bn1.weight\n",
      "online_net.backbone.model.visual.layer1.1.bn1.bias\n",
      "online_net.backbone.model.visual.layer1.1.bn2.weight\n",
      "online_net.backbone.model.visual.layer1.1.bn2.bias\n",
      "online_net.backbone.model.visual.layer1.1.bn3.weight\n",
      "online_net.backbone.model.visual.layer1.1.bn3.bias\n",
      "online_net.backbone.model.visual.layer1.2.bn1.weight\n",
      "online_net.backbone.model.visual.layer1.2.bn1.bias\n",
      "online_net.backbone.model.visual.layer1.2.bn2.weight\n",
      "online_net.backbone.model.visual.layer1.2.bn2.bias\n",
      "online_net.backbone.model.visual.layer1.2.bn3.weight\n",
      "online_net.backbone.model.visual.layer1.2.bn3.bias\n",
      "online_net.backbone.model.visual.layer2.0.bn1.weight\n",
      "online_net.backbone.model.visual.layer2.0.bn1.bias\n",
      "online_net.backbone.model.visual.layer2.0.bn2.weight\n",
      "online_net.backbone.model.visual.layer2.0.bn2.bias\n",
      "online_net.backbone.model.visual.layer2.0.bn3.weight\n",
      "online_net.backbone.model.visual.layer2.0.bn3.bias\n",
      "online_net.backbone.model.visual.layer2.0.downsample.1.weight\n",
      "online_net.backbone.model.visual.layer2.0.downsample.1.bias\n",
      "online_net.backbone.model.visual.layer2.1.bn1.weight\n",
      "online_net.backbone.model.visual.layer2.1.bn1.bias\n",
      "online_net.backbone.model.visual.layer2.1.bn2.weight\n",
      "online_net.backbone.model.visual.layer2.1.bn2.bias\n",
      "online_net.backbone.model.visual.layer2.1.bn3.weight\n",
      "online_net.backbone.model.visual.layer2.1.bn3.bias\n",
      "online_net.backbone.model.visual.layer2.2.bn1.weight\n",
      "online_net.backbone.model.visual.layer2.2.bn1.bias\n",
      "online_net.backbone.model.visual.layer2.2.bn2.weight\n",
      "online_net.backbone.model.visual.layer2.2.bn2.bias\n",
      "online_net.backbone.model.visual.layer2.2.bn3.weight\n",
      "online_net.backbone.model.visual.layer2.2.bn3.bias\n",
      "online_net.backbone.model.visual.layer2.3.bn1.weight\n",
      "online_net.backbone.model.visual.layer2.3.bn1.bias\n",
      "online_net.backbone.model.visual.layer2.3.bn2.weight\n",
      "online_net.backbone.model.visual.layer2.3.bn2.bias\n",
      "online_net.backbone.model.visual.layer2.3.bn3.weight\n",
      "online_net.backbone.model.visual.layer2.3.bn3.bias\n",
      "online_net.backbone.model.visual.layer3.0.bn1.weight\n",
      "online_net.backbone.model.visual.layer3.0.bn1.bias\n",
      "online_net.backbone.model.visual.layer3.0.bn2.weight\n",
      "online_net.backbone.model.visual.layer3.0.bn2.bias\n",
      "online_net.backbone.model.visual.layer3.0.bn3.weight\n",
      "online_net.backbone.model.visual.layer3.0.bn3.bias\n",
      "online_net.backbone.model.visual.layer3.0.downsample.1.weight\n",
      "online_net.backbone.model.visual.layer3.0.downsample.1.bias\n",
      "online_net.backbone.model.visual.layer3.1.bn1.weight\n",
      "online_net.backbone.model.visual.layer3.1.bn1.bias\n",
      "online_net.backbone.model.visual.layer3.1.bn2.weight\n",
      "online_net.backbone.model.visual.layer3.1.bn2.bias\n",
      "online_net.backbone.model.visual.layer3.1.bn3.weight\n",
      "online_net.backbone.model.visual.layer3.1.bn3.bias\n",
      "online_net.backbone.model.visual.layer3.2.bn1.weight\n",
      "online_net.backbone.model.visual.layer3.2.bn1.bias\n",
      "online_net.backbone.model.visual.layer3.2.bn2.weight\n",
      "online_net.backbone.model.visual.layer3.2.bn2.bias\n",
      "online_net.backbone.model.visual.layer3.2.bn3.weight\n",
      "online_net.backbone.model.visual.layer3.2.bn3.bias\n",
      "online_net.backbone.model.visual.layer3.3.bn1.weight\n",
      "online_net.backbone.model.visual.layer3.3.bn1.bias\n",
      "online_net.backbone.model.visual.layer3.3.bn2.weight\n",
      "online_net.backbone.model.visual.layer3.3.bn2.bias\n",
      "online_net.backbone.model.visual.layer3.3.bn3.weight\n",
      "online_net.backbone.model.visual.layer3.3.bn3.bias\n",
      "online_net.backbone.model.visual.layer3.4.bn1.weight\n",
      "online_net.backbone.model.visual.layer3.4.bn1.bias\n",
      "online_net.backbone.model.visual.layer3.4.bn2.weight\n",
      "online_net.backbone.model.visual.layer3.4.bn2.bias\n",
      "online_net.backbone.model.visual.layer3.4.bn3.weight\n",
      "online_net.backbone.model.visual.layer3.4.bn3.bias\n",
      "online_net.backbone.model.visual.layer3.5.bn1.weight\n",
      "online_net.backbone.model.visual.layer3.5.bn1.bias\n",
      "online_net.backbone.model.visual.layer3.5.bn2.weight\n",
      "online_net.backbone.model.visual.layer3.5.bn2.bias\n",
      "online_net.backbone.model.visual.layer3.5.bn3.weight\n",
      "online_net.backbone.model.visual.layer3.5.bn3.bias\n",
      "online_net.backbone.model.visual.layer4.0.bn1.weight\n",
      "online_net.backbone.model.visual.layer4.0.bn1.bias\n",
      "online_net.backbone.model.visual.layer4.0.bn2.weight\n",
      "online_net.backbone.model.visual.layer4.0.bn2.bias\n",
      "online_net.backbone.model.visual.layer4.0.bn3.weight\n",
      "online_net.backbone.model.visual.layer4.0.bn3.bias\n",
      "online_net.backbone.model.visual.layer4.0.downsample.1.weight\n",
      "online_net.backbone.model.visual.layer4.0.downsample.1.bias\n",
      "online_net.backbone.model.visual.layer4.1.bn1.weight\n",
      "online_net.backbone.model.visual.layer4.1.bn1.bias\n",
      "online_net.backbone.model.visual.layer4.1.bn2.weight\n",
      "online_net.backbone.model.visual.layer4.1.bn2.bias\n",
      "online_net.backbone.model.visual.layer4.1.bn3.weight\n",
      "online_net.backbone.model.visual.layer4.1.bn3.bias\n",
      "online_net.backbone.model.visual.layer4.2.bn1.weight\n",
      "online_net.backbone.model.visual.layer4.2.bn1.bias\n",
      "online_net.backbone.model.visual.layer4.2.bn2.weight\n",
      "online_net.backbone.model.visual.layer4.2.bn2.bias\n",
      "online_net.backbone.model.visual.layer4.2.bn3.weight\n",
      "online_net.backbone.model.visual.layer4.2.bn3.bias\n",
      "online_net.backbone.model.transformer.resblocks.0.attn.in_proj_bias\n",
      "online_net.backbone.model.transformer.resblocks.0.attn.out_proj.bias\n",
      "online_net.backbone.model.transformer.resblocks.0.ln_1.bias\n",
      "online_net.backbone.model.transformer.resblocks.0.mlp.c_fc.bias\n",
      "online_net.backbone.model.transformer.resblocks.0.mlp.c_proj.bias\n",
      "online_net.backbone.model.transformer.resblocks.0.ln_2.bias\n",
      "online_net.backbone.model.transformer.resblocks.1.attn.in_proj_bias\n",
      "online_net.backbone.model.transformer.resblocks.1.attn.out_proj.bias\n",
      "online_net.backbone.model.transformer.resblocks.1.ln_1.bias\n",
      "online_net.backbone.model.transformer.resblocks.1.mlp.c_fc.bias\n",
      "online_net.backbone.model.transformer.resblocks.1.mlp.c_proj.bias\n",
      "online_net.backbone.model.transformer.resblocks.1.ln_2.bias\n",
      "online_net.backbone.model.transformer.resblocks.2.attn.in_proj_bias\n",
      "online_net.backbone.model.transformer.resblocks.2.attn.out_proj.bias\n",
      "online_net.backbone.model.transformer.resblocks.2.ln_1.bias\n",
      "online_net.backbone.model.transformer.resblocks.2.mlp.c_fc.bias\n",
      "online_net.backbone.model.transformer.resblocks.2.mlp.c_proj.bias\n",
      "online_net.backbone.model.transformer.resblocks.2.ln_2.bias\n",
      "online_net.backbone.model.transformer.resblocks.3.attn.in_proj_bias\n",
      "online_net.backbone.model.transformer.resblocks.3.attn.out_proj.bias\n",
      "online_net.backbone.model.transformer.resblocks.3.ln_1.bias\n",
      "online_net.backbone.model.transformer.resblocks.3.mlp.c_fc.bias\n",
      "online_net.backbone.model.transformer.resblocks.3.mlp.c_proj.bias\n",
      "online_net.backbone.model.transformer.resblocks.3.ln_2.bias\n",
      "online_net.backbone.model.transformer.resblocks.4.attn.in_proj_bias\n",
      "online_net.backbone.model.transformer.resblocks.4.attn.out_proj.bias\n",
      "online_net.backbone.model.transformer.resblocks.4.ln_1.bias\n",
      "online_net.backbone.model.transformer.resblocks.4.mlp.c_fc.bias\n",
      "online_net.backbone.model.transformer.resblocks.4.mlp.c_proj.bias\n",
      "online_net.backbone.model.transformer.resblocks.4.ln_2.bias\n",
      "online_net.backbone.model.transformer.resblocks.5.attn.in_proj_bias\n",
      "online_net.backbone.model.transformer.resblocks.5.attn.out_proj.bias\n",
      "online_net.backbone.model.transformer.resblocks.5.ln_1.bias\n",
      "online_net.backbone.model.transformer.resblocks.5.mlp.c_fc.bias\n",
      "online_net.backbone.model.transformer.resblocks.5.mlp.c_proj.bias\n",
      "online_net.backbone.model.transformer.resblocks.5.ln_2.bias\n",
      "online_net.backbone.model.transformer.resblocks.6.attn.in_proj_bias\n",
      "online_net.backbone.model.transformer.resblocks.6.attn.out_proj.bias\n",
      "online_net.backbone.model.transformer.resblocks.6.ln_1.bias\n",
      "online_net.backbone.model.transformer.resblocks.6.mlp.c_fc.bias\n",
      "online_net.backbone.model.transformer.resblocks.6.mlp.c_proj.bias\n",
      "online_net.backbone.model.transformer.resblocks.6.ln_2.bias\n",
      "online_net.backbone.model.transformer.resblocks.7.attn.in_proj_bias\n",
      "online_net.backbone.model.transformer.resblocks.7.attn.out_proj.bias\n",
      "online_net.backbone.model.transformer.resblocks.7.ln_1.bias\n",
      "online_net.backbone.model.transformer.resblocks.7.mlp.c_fc.bias\n",
      "online_net.backbone.model.transformer.resblocks.7.mlp.c_proj.bias\n",
      "online_net.backbone.model.transformer.resblocks.7.ln_2.bias\n",
      "online_net.backbone.model.transformer.resblocks.8.attn.in_proj_bias\n",
      "online_net.backbone.model.transformer.resblocks.8.attn.out_proj.bias\n",
      "online_net.backbone.model.transformer.resblocks.8.ln_1.bias\n",
      "online_net.backbone.model.transformer.resblocks.8.mlp.c_fc.bias\n",
      "online_net.backbone.model.transformer.resblocks.8.mlp.c_proj.bias\n",
      "online_net.backbone.model.transformer.resblocks.8.ln_2.bias\n",
      "online_net.backbone.model.transformer.resblocks.9.attn.in_proj_bias\n",
      "online_net.backbone.model.transformer.resblocks.9.attn.out_proj.bias\n",
      "online_net.backbone.model.transformer.resblocks.9.ln_1.bias\n",
      "online_net.backbone.model.transformer.resblocks.9.mlp.c_fc.bias\n",
      "online_net.backbone.model.transformer.resblocks.9.mlp.c_proj.bias\n",
      "online_net.backbone.model.transformer.resblocks.9.ln_2.bias\n",
      "online_net.backbone.model.transformer.resblocks.10.attn.in_proj_bias\n",
      "online_net.backbone.model.transformer.resblocks.10.attn.out_proj.bias\n",
      "online_net.backbone.model.transformer.resblocks.10.ln_1.bias\n",
      "online_net.backbone.model.transformer.resblocks.10.mlp.c_fc.bias\n",
      "online_net.backbone.model.transformer.resblocks.10.mlp.c_proj.bias\n",
      "online_net.backbone.model.transformer.resblocks.10.ln_2.bias\n",
      "online_net.backbone.model.transformer.resblocks.11.attn.in_proj_bias\n",
      "online_net.backbone.model.transformer.resblocks.11.attn.out_proj.bias\n",
      "online_net.backbone.model.transformer.resblocks.11.ln_1.bias\n",
      "online_net.backbone.model.transformer.resblocks.11.mlp.c_fc.bias\n",
      "online_net.backbone.model.transformer.resblocks.11.mlp.c_proj.bias\n",
      "online_net.backbone.model.transformer.resblocks.11.ln_2.bias\n",
      "online_net.backbone.model.ln_final.bias\n",
      "online_net.backbone.feed_forward.mlp.0.bias\n",
      "online_net.backbone.feed_forward.mlp.2.bias\n",
      "online_net.backbone.feed_forward.mlp.4.bias\n",
      "online_net.backbone.feed_forward.mlp.6.bias\n",
      "online_net.neck.proj.mlp.1.bias\n",
      "online_net.neck.proj_obj.mlp.1.bias\n",
      "target_net.backbone.model.visual.bn1.weight\n",
      "target_net.backbone.model.visual.bn1.bias\n",
      "target_net.backbone.model.visual.bn2.weight\n",
      "target_net.backbone.model.visual.bn2.bias\n",
      "target_net.backbone.model.visual.bn3.weight\n",
      "target_net.backbone.model.visual.bn3.bias\n",
      "target_net.backbone.model.visual.layer1.0.bn1.weight\n",
      "target_net.backbone.model.visual.layer1.0.bn1.bias\n",
      "target_net.backbone.model.visual.layer1.0.bn2.weight\n",
      "target_net.backbone.model.visual.layer1.0.bn2.bias\n",
      "target_net.backbone.model.visual.layer1.0.bn3.weight\n",
      "target_net.backbone.model.visual.layer1.0.bn3.bias\n",
      "target_net.backbone.model.visual.layer1.0.downsample.1.weight\n",
      "target_net.backbone.model.visual.layer1.0.downsample.1.bias\n",
      "target_net.backbone.model.visual.layer1.1.bn1.weight\n",
      "target_net.backbone.model.visual.layer1.1.bn1.bias\n",
      "target_net.backbone.model.visual.layer1.1.bn2.weight\n",
      "target_net.backbone.model.visual.layer1.1.bn2.bias\n",
      "target_net.backbone.model.visual.layer1.1.bn3.weight\n",
      "target_net.backbone.model.visual.layer1.1.bn3.bias\n",
      "target_net.backbone.model.visual.layer1.2.bn1.weight\n",
      "target_net.backbone.model.visual.layer1.2.bn1.bias\n",
      "target_net.backbone.model.visual.layer1.2.bn2.weight\n",
      "target_net.backbone.model.visual.layer1.2.bn2.bias\n",
      "target_net.backbone.model.visual.layer1.2.bn3.weight\n",
      "target_net.backbone.model.visual.layer1.2.bn3.bias\n",
      "target_net.backbone.model.visual.layer2.0.bn1.weight\n",
      "target_net.backbone.model.visual.layer2.0.bn1.bias\n",
      "target_net.backbone.model.visual.layer2.0.bn2.weight\n",
      "target_net.backbone.model.visual.layer2.0.bn2.bias\n",
      "target_net.backbone.model.visual.layer2.0.bn3.weight\n",
      "target_net.backbone.model.visual.layer2.0.bn3.bias\n",
      "target_net.backbone.model.visual.layer2.0.downsample.1.weight\n",
      "target_net.backbone.model.visual.layer2.0.downsample.1.bias\n",
      "target_net.backbone.model.visual.layer2.1.bn1.weight\n",
      "target_net.backbone.model.visual.layer2.1.bn1.bias\n",
      "target_net.backbone.model.visual.layer2.1.bn2.weight\n",
      "target_net.backbone.model.visual.layer2.1.bn2.bias\n",
      "target_net.backbone.model.visual.layer2.1.bn3.weight\n",
      "target_net.backbone.model.visual.layer2.1.bn3.bias\n",
      "target_net.backbone.model.visual.layer2.2.bn1.weight\n",
      "target_net.backbone.model.visual.layer2.2.bn1.bias\n",
      "target_net.backbone.model.visual.layer2.2.bn2.weight\n",
      "target_net.backbone.model.visual.layer2.2.bn2.bias\n",
      "target_net.backbone.model.visual.layer2.2.bn3.weight\n",
      "target_net.backbone.model.visual.layer2.2.bn3.bias\n",
      "target_net.backbone.model.visual.layer2.3.bn1.weight\n",
      "target_net.backbone.model.visual.layer2.3.bn1.bias\n",
      "target_net.backbone.model.visual.layer2.3.bn2.weight\n",
      "target_net.backbone.model.visual.layer2.3.bn2.bias\n",
      "target_net.backbone.model.visual.layer2.3.bn3.weight\n",
      "target_net.backbone.model.visual.layer2.3.bn3.bias\n",
      "target_net.backbone.model.visual.layer3.0.bn1.weight\n",
      "target_net.backbone.model.visual.layer3.0.bn1.bias\n",
      "target_net.backbone.model.visual.layer3.0.bn2.weight\n",
      "target_net.backbone.model.visual.layer3.0.bn2.bias\n",
      "target_net.backbone.model.visual.layer3.0.bn3.weight\n",
      "target_net.backbone.model.visual.layer3.0.bn3.bias\n",
      "target_net.backbone.model.visual.layer3.0.downsample.1.weight\n",
      "target_net.backbone.model.visual.layer3.0.downsample.1.bias\n",
      "target_net.backbone.model.visual.layer3.1.bn1.weight\n",
      "target_net.backbone.model.visual.layer3.1.bn1.bias\n",
      "target_net.backbone.model.visual.layer3.1.bn2.weight\n",
      "target_net.backbone.model.visual.layer3.1.bn2.bias\n",
      "target_net.backbone.model.visual.layer3.1.bn3.weight\n",
      "target_net.backbone.model.visual.layer3.1.bn3.bias\n",
      "target_net.backbone.model.visual.layer3.2.bn1.weight\n",
      "target_net.backbone.model.visual.layer3.2.bn1.bias\n",
      "target_net.backbone.model.visual.layer3.2.bn2.weight\n",
      "target_net.backbone.model.visual.layer3.2.bn2.bias\n",
      "target_net.backbone.model.visual.layer3.2.bn3.weight\n",
      "target_net.backbone.model.visual.layer3.2.bn3.bias\n",
      "target_net.backbone.model.visual.layer3.3.bn1.weight\n",
      "target_net.backbone.model.visual.layer3.3.bn1.bias\n",
      "target_net.backbone.model.visual.layer3.3.bn2.weight\n",
      "target_net.backbone.model.visual.layer3.3.bn2.bias\n",
      "target_net.backbone.model.visual.layer3.3.bn3.weight\n",
      "target_net.backbone.model.visual.layer3.3.bn3.bias\n",
      "target_net.backbone.model.visual.layer3.4.bn1.weight\n",
      "target_net.backbone.model.visual.layer3.4.bn1.bias\n",
      "target_net.backbone.model.visual.layer3.4.bn2.weight\n",
      "target_net.backbone.model.visual.layer3.4.bn2.bias\n",
      "target_net.backbone.model.visual.layer3.4.bn3.weight\n",
      "target_net.backbone.model.visual.layer3.4.bn3.bias\n",
      "target_net.backbone.model.visual.layer3.5.bn1.weight\n",
      "target_net.backbone.model.visual.layer3.5.bn1.bias\n",
      "target_net.backbone.model.visual.layer3.5.bn2.weight\n",
      "target_net.backbone.model.visual.layer3.5.bn2.bias\n",
      "target_net.backbone.model.visual.layer3.5.bn3.weight\n",
      "target_net.backbone.model.visual.layer3.5.bn3.bias\n",
      "target_net.backbone.model.visual.layer4.0.bn1.weight\n",
      "target_net.backbone.model.visual.layer4.0.bn1.bias\n",
      "target_net.backbone.model.visual.layer4.0.bn2.weight\n",
      "target_net.backbone.model.visual.layer4.0.bn2.bias\n",
      "target_net.backbone.model.visual.layer4.0.bn3.weight\n",
      "target_net.backbone.model.visual.layer4.0.bn3.bias\n",
      "target_net.backbone.model.visual.layer4.0.downsample.1.weight\n",
      "target_net.backbone.model.visual.layer4.0.downsample.1.bias\n",
      "target_net.backbone.model.visual.layer4.1.bn1.weight\n",
      "target_net.backbone.model.visual.layer4.1.bn1.bias\n",
      "target_net.backbone.model.visual.layer4.1.bn2.weight\n",
      "target_net.backbone.model.visual.layer4.1.bn2.bias\n",
      "target_net.backbone.model.visual.layer4.1.bn3.weight\n",
      "target_net.backbone.model.visual.layer4.1.bn3.bias\n",
      "target_net.backbone.model.visual.layer4.2.bn1.weight\n",
      "target_net.backbone.model.visual.layer4.2.bn1.bias\n",
      "target_net.backbone.model.visual.layer4.2.bn2.weight\n",
      "target_net.backbone.model.visual.layer4.2.bn2.bias\n",
      "target_net.backbone.model.visual.layer4.2.bn3.weight\n",
      "target_net.backbone.model.visual.layer4.2.bn3.bias\n",
      "target_net.backbone.model.transformer.resblocks.0.attn.in_proj_bias\n",
      "target_net.backbone.model.transformer.resblocks.0.attn.out_proj.bias\n",
      "target_net.backbone.model.transformer.resblocks.0.ln_1.bias\n",
      "target_net.backbone.model.transformer.resblocks.0.mlp.c_fc.bias\n",
      "target_net.backbone.model.transformer.resblocks.0.mlp.c_proj.bias\n",
      "target_net.backbone.model.transformer.resblocks.0.ln_2.bias\n",
      "target_net.backbone.model.transformer.resblocks.1.attn.in_proj_bias\n",
      "target_net.backbone.model.transformer.resblocks.1.attn.out_proj.bias\n",
      "target_net.backbone.model.transformer.resblocks.1.ln_1.bias\n",
      "target_net.backbone.model.transformer.resblocks.1.mlp.c_fc.bias\n",
      "target_net.backbone.model.transformer.resblocks.1.mlp.c_proj.bias\n",
      "target_net.backbone.model.transformer.resblocks.1.ln_2.bias\n",
      "target_net.backbone.model.transformer.resblocks.2.attn.in_proj_bias\n",
      "target_net.backbone.model.transformer.resblocks.2.attn.out_proj.bias\n",
      "target_net.backbone.model.transformer.resblocks.2.ln_1.bias\n",
      "target_net.backbone.model.transformer.resblocks.2.mlp.c_fc.bias\n",
      "target_net.backbone.model.transformer.resblocks.2.mlp.c_proj.bias\n",
      "target_net.backbone.model.transformer.resblocks.2.ln_2.bias\n",
      "target_net.backbone.model.transformer.resblocks.3.attn.in_proj_bias\n",
      "target_net.backbone.model.transformer.resblocks.3.attn.out_proj.bias\n",
      "target_net.backbone.model.transformer.resblocks.3.ln_1.bias\n",
      "target_net.backbone.model.transformer.resblocks.3.mlp.c_fc.bias\n",
      "target_net.backbone.model.transformer.resblocks.3.mlp.c_proj.bias\n",
      "target_net.backbone.model.transformer.resblocks.3.ln_2.bias\n",
      "target_net.backbone.model.transformer.resblocks.4.attn.in_proj_bias\n",
      "target_net.backbone.model.transformer.resblocks.4.attn.out_proj.bias\n",
      "target_net.backbone.model.transformer.resblocks.4.ln_1.bias\n",
      "target_net.backbone.model.transformer.resblocks.4.mlp.c_fc.bias\n",
      "target_net.backbone.model.transformer.resblocks.4.mlp.c_proj.bias\n",
      "target_net.backbone.model.transformer.resblocks.4.ln_2.bias\n",
      "target_net.backbone.model.transformer.resblocks.5.attn.in_proj_bias\n",
      "target_net.backbone.model.transformer.resblocks.5.attn.out_proj.bias\n",
      "target_net.backbone.model.transformer.resblocks.5.ln_1.bias\n",
      "target_net.backbone.model.transformer.resblocks.5.mlp.c_fc.bias\n",
      "target_net.backbone.model.transformer.resblocks.5.mlp.c_proj.bias\n",
      "target_net.backbone.model.transformer.resblocks.5.ln_2.bias\n",
      "target_net.backbone.model.transformer.resblocks.6.attn.in_proj_bias\n",
      "target_net.backbone.model.transformer.resblocks.6.attn.out_proj.bias\n",
      "target_net.backbone.model.transformer.resblocks.6.ln_1.bias\n",
      "target_net.backbone.model.transformer.resblocks.6.mlp.c_fc.bias\n",
      "target_net.backbone.model.transformer.resblocks.6.mlp.c_proj.bias\n",
      "target_net.backbone.model.transformer.resblocks.6.ln_2.bias\n",
      "target_net.backbone.model.transformer.resblocks.7.attn.in_proj_bias\n",
      "target_net.backbone.model.transformer.resblocks.7.attn.out_proj.bias\n",
      "target_net.backbone.model.transformer.resblocks.7.ln_1.bias\n",
      "target_net.backbone.model.transformer.resblocks.7.mlp.c_fc.bias\n",
      "target_net.backbone.model.transformer.resblocks.7.mlp.c_proj.bias\n",
      "target_net.backbone.model.transformer.resblocks.7.ln_2.bias\n",
      "target_net.backbone.model.transformer.resblocks.8.attn.in_proj_bias\n",
      "target_net.backbone.model.transformer.resblocks.8.attn.out_proj.bias\n",
      "target_net.backbone.model.transformer.resblocks.8.ln_1.bias\n",
      "target_net.backbone.model.transformer.resblocks.8.mlp.c_fc.bias\n",
      "target_net.backbone.model.transformer.resblocks.8.mlp.c_proj.bias\n",
      "target_net.backbone.model.transformer.resblocks.8.ln_2.bias\n",
      "target_net.backbone.model.transformer.resblocks.9.attn.in_proj_bias\n",
      "target_net.backbone.model.transformer.resblocks.9.attn.out_proj.bias\n",
      "target_net.backbone.model.transformer.resblocks.9.ln_1.bias\n",
      "target_net.backbone.model.transformer.resblocks.9.mlp.c_fc.bias\n",
      "target_net.backbone.model.transformer.resblocks.9.mlp.c_proj.bias\n",
      "target_net.backbone.model.transformer.resblocks.9.ln_2.bias\n",
      "target_net.backbone.model.transformer.resblocks.10.attn.in_proj_bias\n",
      "target_net.backbone.model.transformer.resblocks.10.attn.out_proj.bias\n",
      "target_net.backbone.model.transformer.resblocks.10.ln_1.bias\n",
      "target_net.backbone.model.transformer.resblocks.10.mlp.c_fc.bias\n",
      "target_net.backbone.model.transformer.resblocks.10.mlp.c_proj.bias\n",
      "target_net.backbone.model.transformer.resblocks.10.ln_2.bias\n",
      "target_net.backbone.model.transformer.resblocks.11.attn.in_proj_bias\n",
      "target_net.backbone.model.transformer.resblocks.11.attn.out_proj.bias\n",
      "target_net.backbone.model.transformer.resblocks.11.ln_1.bias\n",
      "target_net.backbone.model.transformer.resblocks.11.mlp.c_fc.bias\n",
      "target_net.backbone.model.transformer.resblocks.11.mlp.c_proj.bias\n",
      "target_net.backbone.model.transformer.resblocks.11.ln_2.bias\n",
      "target_net.backbone.model.ln_final.bias\n",
      "target_net.backbone.feed_forward.mlp.0.bias\n",
      "target_net.backbone.feed_forward.mlp.2.bias\n",
      "target_net.backbone.feed_forward.mlp.4.bias\n",
      "target_net.backbone.feed_forward.mlp.6.bias\n",
      "target_net.neck.proj.mlp.1.bias\n",
      "target_net.neck.proj_obj.mlp.1.bias\n",
      "predictor.mlp.1.bias\n",
      "predictor_obj.mlp.1.bias\n",
      "train_dataset:\n",
      "<data.base_dataset.DownstreamDataset object at 0x0000023D45283100>\n",
      "Start the training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chinm\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\amp\\autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/150]\t[  0/368]\tLR 2.7173913043478264e-07\tMOM 0.996\tTime 52.955 (52.955)\tData 21.725 (21.725)\tLoss 3.9927e+00 (3.9927e+00)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Independent Study\\LEWEL\\Train.ipynb Cell 8\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Independent%20Study/LEWEL/Train.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m confs[\u001b[39m\"\u001b[39m\u001b[39mmultiprocessing_distributed\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Independent%20Study/LEWEL/Train.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m cudnn\u001b[39m.\u001b[39mbenchmark \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Independent%20Study/LEWEL/Train.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m main()\n",
      "\u001b[1;32md:\\Independent Study\\LEWEL\\Train.ipynb Cell 8\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Independent%20Study/LEWEL/Train.ipynb#X10sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m     lr_schedule\u001b[39m.\u001b[39madjust_learning_rate(optimizer, epoch, confs)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Independent%20Study/LEWEL/Train.ipynb#X10sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m \u001b[39m# train for one epoch\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/Independent%20Study/LEWEL/Train.ipynb#X10sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m train(train_loader, model, optimizer, scaler, epoch, confs)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Independent%20Study/LEWEL/Train.ipynb#X10sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m \u001b[39mif\u001b[39;00m (epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m confs[\u001b[39m\"\u001b[39m\u001b[39meval_freq\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/Independent%20Study/LEWEL/Train.ipynb#X10sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m     val_loss \u001b[39m=\u001b[39m validate(val_loader_base , model,confs)\n",
      "\u001b[1;32md:\\Independent Study\\LEWEL\\Train.ipynb Cell 8\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_loader, model, optimizer, scaler, epoch, args)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Independent%20Study/LEWEL/Train.ipynb#X10sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mif\u001b[39;00m scaler \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Independent%20Study/LEWEL/Train.ipynb#X10sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39m# compute loss\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Independent%20Study/LEWEL/Train.ipynb#X10sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Independent%20Study/LEWEL/Train.ipynb#X10sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m         loss \u001b[39m=\u001b[39m model(im_v1\u001b[39m=\u001b[39;49mimages[\u001b[39m0\u001b[39;49m], im_v2\u001b[39m=\u001b[39;49mimages[\u001b[39m1\u001b[39;49m] , caption \u001b[39m=\u001b[39;49m caption, idx\u001b[39m=\u001b[39;49midx)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Independent%20Study/LEWEL/Train.ipynb#X10sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39m# measure accuracy and record loss\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Independent%20Study/LEWEL/Train.ipynb#X10sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     losses\u001b[39m.\u001b[39mupdate(loss\u001b[39m.\u001b[39mitem(), images[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\chinm\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Independent Study\\LEWEL\\models\\lewel.py:201\u001b[0m, in \u001b[0;36mLEWELB_EMAN.forward\u001b[1;34m(self, im_v1, im_v2, caption, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():  \u001b[39m# no gradient to keys\u001b[39;00m\n\u001b[0;32m    200\u001b[0m     proj_target_v1 \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mdetach() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_net(im_v1,caption)]\n\u001b[1;32m--> 201\u001b[0m     proj_target_v2 \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mdetach() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_net(im_v2,caption)]\n\u001b[0;32m    203\u001b[0m \u001b[39m# loss. NOTE: the predction is moved to loss_func\u001b[39;00m\n\u001b[0;32m    204\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_func(proj_online_v1, proj_target_v2) \u001b[39m+\u001b[39m \\\n\u001b[0;32m    205\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_func(proj_online_v2, proj_target_v1)\n",
      "File \u001b[1;32mc:\\Users\\chinm\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Independent Study\\LEWEL\\models\\lewel.py:107\u001b[0m, in \u001b[0;36mEncoderObj.forward\u001b[1;34m(self, im, caption)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, im, caption):\n\u001b[1;32m--> 107\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(im, caption)\n\u001b[0;32m    108\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneck(out)\n\u001b[0;32m    109\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\chinm\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Independent Study\\LEWEL\\backbone\\my_clip.py:36\u001b[0m, in \u001b[0;36mCLIP.forward\u001b[1;34m(self, image, caption)\u001b[0m\n\u001b[0;32m     34\u001b[0m image_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mencode_image(image)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     35\u001b[0m text_features \u001b[39m=\u001b[39m clip\u001b[39m.\u001b[39mtokenize(caption,truncate \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m---> 36\u001b[0m text_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mencode_text(text_features)\n\u001b[0;32m     37\u001b[0m text_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed_forward(text_features)\n\u001b[0;32m     38\u001b[0m text_features \u001b[39m=\u001b[39m text_features\u001b[39m.\u001b[39mreshape(text_features\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],\u001b[39m7\u001b[39m,\u001b[39m7\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\chinm\\anaconda3\\envs\\pytorch\\lib\\site-packages\\clip\\model.py:348\u001b[0m, in \u001b[0;36mCLIP.encode_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    346\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_embedding\u001b[39m.\u001b[39mtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m    347\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# NLD -> LND\u001b[39;00m\n\u001b[1;32m--> 348\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(x)\n\u001b[0;32m    349\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)  \u001b[39m# LND -> NLD\u001b[39;00m\n\u001b[0;32m    350\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_final(x)\u001b[39m.\u001b[39mtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\chinm\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\chinm\\anaconda3\\envs\\pytorch\\lib\\site-packages\\clip\\model.py:203\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m--> 203\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresblocks(x)\n",
      "File \u001b[1;32mc:\\Users\\chinm\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\chinm\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\chinm\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\chinm\\anaconda3\\envs\\pytorch\\lib\\site-packages\\clip\\model.py:190\u001b[0m, in \u001b[0;36mResidualAttentionBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m--> 190\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln_1(x))\n\u001b[0;32m    191\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2(x))\n\u001b[0;32m    192\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\chinm\\anaconda3\\envs\\pytorch\\lib\\site-packages\\clip\\model.py:187\u001b[0m, in \u001b[0;36mResidualAttentionBlock.attention\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mattention\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m    186\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn_mask\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mdevice) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(x, x, x, need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, attn_mask\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn_mask)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\chinm\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\chinm\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1153\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights)\u001b[0m\n\u001b[0;32m   1142\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1143\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[0;32m   1144\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1150\u001b[0m         q_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj_weight, k_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj_weight,\n\u001b[0;32m   1151\u001b[0m         v_proj_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_proj_weight, average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights)\n\u001b[0;32m   1152\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1153\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[0;32m   1154\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[0;32m   1155\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[0;32m   1156\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[0;32m   1157\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m   1158\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[0;32m   1159\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask, need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[0;32m   1160\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask, average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights)\n\u001b[0;32m   1161\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[0;32m   1162\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32mc:\\Users\\chinm\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:5120\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights)\u001b[0m\n\u001b[0;32m   5115\u001b[0m     \u001b[39massert\u001b[39;00m bias_v \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   5117\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m   5118\u001b[0m \u001b[39m# reshape q, k, v for multihead attention and make em batch first\u001b[39;00m\n\u001b[0;32m   5119\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m-> 5120\u001b[0m q \u001b[39m=\u001b[39m q\u001b[39m.\u001b[39;49mcontiguous()\u001b[39m.\u001b[39mview(tgt_len, bsz \u001b[39m*\u001b[39m num_heads, head_dim)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m   5121\u001b[0m \u001b[39mif\u001b[39;00m static_k \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   5122\u001b[0m     k \u001b[39m=\u001b[39m k\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(k\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], bsz \u001b[39m*\u001b[39m num_heads, head_dim)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    device = torch.cuda.current_device() if torch.cuda.is_available() else \"cpu\"\n",
    "    confs[\"distributed\"] = False\n",
    "    confs[\"multiprocessing_distributed\"] = False\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 | packaged by conda-forge | (main, Mar 30 2022, 08:38:02) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "5ba1aea00085fcf9477236538efe0cd8e2de1f07aa20f35434921a054985b396"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
